"""
å¤šçº¿ç¨‹å¼‚æ­¥ä¸‹è½½å·¥å…·
æ”¯æŒå•æ–‡ä»¶ä¸‹è½½ã€æ‰¹é‡ä¸‹è½½ã€æ–­ç‚¹ç»­ä¼ ç­‰åŠŸèƒ½
"""
import asyncio
import aiohttp
import aiofiles
import os
from pathlib import Path
from typing import List, Union, Optional, Dict, Any
from urllib.parse import urlparse
import time
from loguru import logger


class AsyncDownloader:
    """å¼‚æ­¥ä¸‹è½½å™¨"""

    def __init__(self,
                 max_concurrent: int = 10,
                 chunk_size: int = 8192,
                 timeout: int = 300,
                 headers: Optional[Dict[str, str]] = None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            max_concurrent: æœ€å¤§å¹¶å‘ä¸‹è½½æ•°
            chunk_size: æ¯æ¬¡è¯»å–çš„å—å¤§å°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            headers: è¯·æ±‚å¤´
        """
        self.max_concurrent = max_concurrent
        self.chunk_size = chunk_size
        self.timeout_seconds = timeout
        self.headers = headers or {}

    async def download_single(self,
                              url: str,
                              save_path: Union[str, Path],
                              progress_callback: Optional[callable] = None) -> Dict[str, Any]:
        """
        ä¸‹è½½å•ä¸ªæ–‡ä»¶
        
        Args:
            url: ä¸‹è½½URL
            save_path: ä¿å­˜è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, url)
            
        Returns:
            Dict: ä¸‹è½½ç»“æœä¿¡æ¯
        """
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)

        start_time = time.time()

        try:
            # åœ¨å¼‚æ­¥å‡½æ•°å†…åˆ›å»ºè¿æ¥å™¨å’Œè¶…æ—¶é…ç½®
            connector = aiohttp.TCPConnector(
                limit=self.max_concurrent + 10,
                limit_per_host=self.max_concurrent + 10,
                keepalive_timeout=60,
                enable_cleanup_closed=True,
                use_dns_cache=True,
                ttl_dns_cache=300,
            )

            timeout = aiohttp.ClientTimeout(
                total=self.timeout_seconds,
                connect=10,
                sock_read=30
            )

            # åˆ›å»ºä¼šè¯
            async with aiohttp.ClientSession(
                    connector=connector,
                    timeout=timeout,
                    headers=self.headers
            ) as session:

                async with session.get(url) as response:
                    if response.status != 200:
                        raise Exception(f"HTTP {response.status}: {response.reason}")

                    total_size = int(response.headers.get('content-length', 0))
                    downloaded = 0

                    async with aiofiles.open(save_path, 'wb') as f:
                        async for chunk in response.content.iter_chunked(self.chunk_size):
                            await f.write(chunk)
                            downloaded += len(chunk)

                            if progress_callback and total_size > 0:
                                progress_callback(downloaded, total_size, url)

                    elapsed = time.time() - start_time
                    speed = downloaded / elapsed if elapsed > 0 else 0

                    logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {url} -> {save_path} ({downloaded} bytes, {speed:.2f} KB/s)")

                    return {
                        "success": True,
                        "url": url,
                        "save_path": str(save_path),
                        "size": downloaded,
                        "elapsed": elapsed,
                        "speed": speed
                    }

        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {url} - {e}")
            return {
                "success": False,
                "url": url,
                "save_path": str(save_path),
                "error": str(e)
            }

    async def download_batch(self,
                             download_list: List[Dict[str, str]],
                             progress_callback: Optional[callable] = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡ä¸‹è½½æ–‡ä»¶
        
        Args:
            download_list: ä¸‹è½½åˆ—è¡¨ï¼Œæ ¼å¼: [{"url": "xxx", "save_path": "xxx"}, ...]
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (completed, total, current_url)
            
        Returns:
            List[Dict]: æ‰€æœ‰ä¸‹è½½ç»“æœ
        """
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def download_with_semaphore(item):
            async with semaphore:
                return await self.download_single(
                    item["url"],
                    item["save_path"],
                    progress_callback
                )

        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(download_list)} ä¸ªæ–‡ä»¶...")
        start_time = time.time()

        tasks = [download_with_semaphore(item) for item in download_list]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "success": False,
                    "url": download_list[i]["url"],
                    "save_path": download_list[i]["save_path"],
                    "error": str(result)
                })
            else:
                processed_results.append(result)

        elapsed = time.time() - start_time
        successful = [r for r in processed_results if r["success"]]
        failed = [r for r in processed_results if not r["success"]]

        logger.info(f"ğŸ‰ æ‰¹é‡ä¸‹è½½å®Œæˆ! æˆåŠŸ: {len(successful)}, å¤±è´¥: {len(failed)}, è€—æ—¶: {elapsed:.2f}s")

        return processed_results

    def get_filename_from_url(self, url: str) -> str:
        """ä»URLæå–æ–‡ä»¶å"""
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        return filename if filename else "download_file"


# å…¨å±€ä¸‹è½½å™¨å®ä¾‹
downloader = AsyncDownloader()


async def download_file(url: str, save_path: Union[str, Path]) -> Dict[str, Any]:
    """
    ç®€å•çš„å•æ–‡ä»¶ä¸‹è½½å‡½æ•°
    
    Args:
        url: ä¸‹è½½URL
        save_path: ä¿å­˜è·¯å¾„
        
    Returns:
        Dict: ä¸‹è½½ç»“æœ
    """
    return await downloader.download_single(url, save_path)


async def download_files(urls: List[str], save_dir: Union[str, Path]) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡ä¸‹è½½æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•
    
    Args:
        urls: URLåˆ—è¡¨
        save_dir: ä¿å­˜ç›®å½•
        
    Returns:
        List[Dict]: ä¸‹è½½ç»“æœåˆ—è¡¨
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    download_list = []
    for i, url in enumerate(urls):
        filename = downloader.get_filename_from_url(url)
        if not filename or filename == "download_file":
            filename = f"file_{i:03d}"

        download_list.append({
            "url": url,
            "save_path": save_dir / filename
        })

    return await downloader.download_batch(download_list)


# æµ‹è¯•å‡½æ•°
async def test_download():
    """æµ‹è¯•ä¸‹è½½åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ä¸‹è½½åŠŸèƒ½...")

    # æµ‹è¯•å•æ–‡ä»¶ä¸‹è½½ - ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æµ‹è¯•URL
    test_url = "http://8.153.175.16:9001/api/v1/download-shared-object/aHR0cDovLzEyNy4wLjAuMTo5MDAwL2RvY3VtZW50cy8lRTQlQjklOUQlRTQlQjglODklRTklOTglODUlRTUlODUlQjUtQUklRTYlQUQlQTYlRTUlOTklQTgucHB0eD9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPTVMRTc3UzdGOUZEVkdEMzRYTzg5JTJGMjAyNTA5MTIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwOTEyVDAyNTc0N1omWC1BbXotRXhwaXJlcz00MzIwMCZYLUFtei1TZWN1cml0eS1Ub2tlbj1leUpoYkdjaU9pSklVelV4TWlJc0luUjVjQ0k2SWtwWFZDSjkuZXlKaFkyTmxjM05MWlhraU9pSTFURVUzTjFNM1JqbEdSRlpIUkRNMFdFODRPU0lzSW1WNGNDSTZNVGMxTnpZNE5qVTJPU3dpY0dGeVpXNTBJam9pYldsdWFXOWhaRzFwYmlKOS4xY1JmWEJTSWJnT3dmdUI4OXRlczB3MFlQanFRLXFBeW1mMk5CS0lwWWhCbnd2TUtsUUl2d25JLVdNSGUxOXNDSWJ2aGY4bDhtVG5aZ25NWmNuckdMUSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmdmVyc2lvbklkPW51bGwmWC1BbXotU2lnbmF0dXJlPTcxY2E5YzViMTU5N2Y1OGZkMjc5NzkzN2MyNWUwNzY5ZmQ1ZTUxZjEyNmQ1Njk3N2I1MmY3MmE5Mzg3OWI0ZTA"  # 1KBæµ‹è¯•æ–‡ä»¶
    result = await download_file(test_url, "1.pptx")
    print(f"å•æ–‡ä»¶ä¸‹è½½ç»“æœ: {result}")

    # æµ‹è¯•æ‰¹é‡ä¸‹è½½
    # test_urls = [
    #     "https://httpbin.org/bytes/512",
    #     "https://httpbin.org/bytes/1024",
    #     "https://httpbin.org/bytes/2048"
    # ]
    # results = await download_files(test_urls, "test_downloads")
    # print(f"æ‰¹é‡ä¸‹è½½ç»“æœ: {len(results)} ä¸ªæ–‡ä»¶")


if __name__ == "__main__":
    asyncio.run(test_download())
